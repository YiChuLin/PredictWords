{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Generator Using LSTM\n",
    "\n",
    "This script implements a RNN using LSTM cells in Tensorflow. The network is then trained on a set of english vocabularies (https://github.com/dwyl/english-words), the resulting network could produce reasonable vocabulary-like words given the first few characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the dictionary\n",
    "def load_words():\n",
    "    with open('words_alpha.txt') as word_file:\n",
    "        valid_words = word_file.read().split()\n",
    "    return valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapper that maps character to its one hot representation\n",
    "class one_hot_mapper():\n",
    "    \"\"\"\n",
    "    Sample Usage:\n",
    "    >>>mapper = one_hot_mapper(classes = 3)\n",
    "    >>>one_hot = mapper.transform(2)\n",
    "    >>one_hot\n",
    "    [0,1,0]\n",
    "    >>>one_hot_rev = mapper.reverse(one_hot)\n",
    "    >>>one_hot_rev\n",
    "    2\n",
    "    >>>\n",
    "    \"\"\"    \n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "    def transform(self, value):\n",
    "        arr = [0]*self.classes\n",
    "        arr[value] = 1\n",
    "        return arr\n",
    "    def reverse(self, array, int_to_char = None):\n",
    "        if int_to_char:\n",
    "            string = ''\n",
    "            for element in array:\n",
    "                string = string + int_to_char[np.argmax(element)]\n",
    "            return string\n",
    "        else:\n",
    "            return np.argmax(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relevant mappers\n",
    "values = '\\nabcdefghijklmnopqrstuvwxyz'\n",
    "mapper = one_hot_mapper(classes = len(values))\n",
    "\n",
    "char_to_one_hot = dict((key, mapper.transform(value)) for value, key in enumerate(values))\n",
    "int_to_char = dict((mapper.reverse(key), value) for value, key in char_to_one_hot.items())\n",
    "\n",
    "english_words = load_words()\n",
    "max_word_length = max([len(word) for word in english_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the data from the dictionary\n",
    "use_load_file = False\n",
    "\n",
    "if use_load_file == True:\n",
    "    one_hot_encoded = np.load('one_hot_v2.npy')\n",
    "else:\n",
    "    one_hot_encoded = [[char_to_one_hot[char] for char in word] for word in english_words]\n",
    "    change_line_int = char_to_one_hot['\\n']\n",
    "    null_int = change_line_int[:]; null_int[0] = 0\n",
    "    one_hot_encoded =[np.concatenate((word, [change_line_int] * (max_word_length + 1 - len(word))), axis = 0)\n",
    "                                                                            for word in one_hot_encoded]\n",
    "    one_hot_encoded = np.array(one_hot_encoded)\n",
    "    np.save('one_hot_v2.npy', one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370099, 32, 27)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size):\n",
    "    \"\"\"\n",
    "    Yields mini batches with the given size sampling from arr\n",
    "    Sample Usage:\n",
    "    for x,y in get_batches(arr, batch_size):\n",
    "    \"\"\"    \n",
    "    np.random.shuffle(arr)\n",
    "    n_batches = arr.shape[0]//batch_size\n",
    "\n",
    "    for n in range(0, n_batches*batch_size, batch_size):\n",
    "        x = arr[n:n+batch_size]\n",
    "        y_temp = x[:, 1:]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, values, numclasses, top_n=5):\n",
    "    \"\"\"\n",
    "    Sample the top n possible classes given the required inputs\n",
    "    Inputs: preds -> probability array, values -> corresponding values array, numclasses -> number of classes, \n",
    "    typically numclasses = len(values) top_n -> sample from the top_n most probable results(default:5)\n",
    "    Usage: pick_top_n(preds, values, numclasses, top_n = 3)\n",
    "    Return: the sampled value\n",
    "    \"\"\"    \n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p/np.sum(p)\n",
    "    c = np.random.choice(numclasses, 1, p=p)[0]\n",
    "    return values[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, max_samples, num_classes, prime = 'a'):\n",
    "    \"\"\"\n",
    "    Sample with the given checkpoint\n",
    "    Input: checkpoint, max_samples, num_classes, prime (default:'a')\n",
    "    Sample Usage:\n",
    "    samp = sample(checkpoint, max_samples, num_classes, prime = 'jupyt')\n",
    "    \"\"\"    \n",
    "    samples = prime\n",
    "    model = VocabRNN(learning_rate = learning_rate, sampling = True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            #x = np.zeros((1,1))\n",
    "            x = np.array([char_to_one_hot[c]]).reshape((1,1,-1))\n",
    "            feed_dict = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], feed_dict = feed_dict)\n",
    "        c = pick_top_n(preds, values, num_classes)\n",
    "        samples = samples + c\n",
    "        if c != '\\n':\n",
    "            for n in range(max_samples):\n",
    "                x = np.array([char_to_one_hot[c]]).reshape((1,1,-1))\n",
    "                feed_dict = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                            feed_dict = feed_dict)\n",
    "                c = pick_top_n(preds, values, num_classes)\n",
    "                samples = samples + c\n",
    "                if c == '\\n':\n",
    "                    break\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RNN\n",
    "To build the RNN we first define a build_cell() function that returns a basic lstm cell with dropout. Then in the RNN class use tf.contrib.rnn.MultiRNNCell() along with build_cell to build multiple layers of the network. For the optimizer we perform gradient clipping of adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cell(lstm_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Returns a basic lstm cell with dripout.\n",
    "    Inputs: lstm_size, keep_prob\n",
    "    Output: tensorflow LSTMCell with DropoutWrapper\n",
    "    \"\"\"\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size, name = 'basic_lstm_cell')\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "    return drop\n",
    "def grad_clip_adam(loss, learning_rate, grad_clip):\n",
    "    \"\"\"\n",
    "    Returns an adam optimizer that is gradient clipped.\n",
    "    Inputs: loss, learning_rate, grad_clip\n",
    "    \"\"\"\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabRNN:\n",
    "    def __init__(self, num_classes = 27, batch_size = 256, num_steps = 32, \n",
    "                    learning_rate = 0.001, grad_clip = 5, sampling = False):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.inputs = tf.placeholder(tf.float32, [batch_size, num_steps, num_classes], name='inputs')\n",
    "        self.targets = tf.placeholder(tf.float32, [batch_size, num_steps, num_classes], name='targets')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_cell(128, self.keep_prob) for _ in range(3)]) #lstm_size, num_layers\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, self.inputs, initial_state = self.initial_state)\n",
    "        self.final_state = state\n",
    "\n",
    "        seq_output = tf.concat(outputs, axis=1)\n",
    "        x = tf.reshape(seq_output, [-1, 128]) #lstm_size\n",
    "\n",
    "        self.logits = tf.layers.dense(x, num_classes)\n",
    "        self.prediction = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.targets)\n",
    "        self.loss = tf.reduce_mean(self.loss)\n",
    "        self.optimizer = grad_clip_adam(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Hyper-parameters and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(values)\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.8\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "save_freq = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = VocabRNN(learning_rate = learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Training Step: 200... Training loss: 1.3836... \n",
      "Epoch: 1/20... Training Step: 400... Training loss: 1.3742... \n",
      "Epoch: 1/20... Training Step: 600... Training loss: 1.3709... \n",
      "Epoch: 1/20... Training Step: 800... Training loss: 1.3593... \n",
      "Epoch: 1/20... Training Step: 1000... Training loss: 1.3253... \n",
      "Epoch: 1/20... Training Step: 1200... Training loss: 1.3181... \n",
      "Epoch: 1/20... Training Step: 1400... Training loss: 1.3749... \n",
      "Epoch: 2/20... Training Step: 1600... Training loss: 1.2949... \n",
      "Epoch: 2/20... Training Step: 1800... Training loss: 1.3323... \n",
      "Epoch: 2/20... Training Step: 2000... Training loss: 1.3319... \n",
      "Epoch: 2/20... Training Step: 2200... Training loss: 1.3755... \n",
      "Epoch: 2/20... Training Step: 2400... Training loss: 1.3269... \n",
      "Epoch: 2/20... Training Step: 2600... Training loss: 1.3569... \n",
      "Epoch: 2/20... Training Step: 2800... Training loss: 1.3459... \n",
      "Epoch: 3/20... Training Step: 3000... Training loss: 1.2603... \n",
      "Epoch: 3/20... Training Step: 3200... Training loss: 1.0800... \n",
      "Epoch: 3/20... Training Step: 3400... Training loss: 0.9241... \n",
      "Epoch: 3/20... Training Step: 3600... Training loss: 0.8917... \n",
      "Epoch: 3/20... Training Step: 3800... Training loss: 0.9003... \n",
      "Epoch: 3/20... Training Step: 4000... Training loss: 0.8572... \n",
      "Epoch: 3/20... Training Step: 4200... Training loss: 0.8570... \n",
      "Epoch: 4/20... Training Step: 4400... Training loss: 0.8023... \n",
      "Epoch: 4/20... Training Step: 4600... Training loss: 0.8532... \n",
      "Epoch: 4/20... Training Step: 4800... Training loss: 0.8418... \n",
      "Epoch: 4/20... Training Step: 5000... Training loss: 0.8276... \n",
      "Epoch: 4/20... Training Step: 5200... Training loss: 0.8402... \n",
      "Epoch: 4/20... Training Step: 5400... Training loss: 0.8414... \n",
      "Epoch: 4/20... Training Step: 5600... Training loss: 0.8473... \n",
      "Epoch: 5/20... Training Step: 5800... Training loss: 0.8820... \n",
      "Epoch: 5/20... Training Step: 6000... Training loss: 0.8337... \n",
      "Epoch: 5/20... Training Step: 6200... Training loss: 0.8619... \n",
      "Epoch: 5/20... Training Step: 6400... Training loss: 0.8617... \n",
      "Epoch: 5/20... Training Step: 6600... Training loss: 0.8442... \n",
      "Epoch: 5/20... Training Step: 6800... Training loss: 0.8327... \n",
      "Epoch: 5/20... Training Step: 7000... Training loss: 0.8525... \n",
      "Epoch: 5/20... Training Step: 7200... Training loss: 0.8862... \n",
      "Epoch: 6/20... Training Step: 7400... Training loss: 0.8465... \n",
      "Epoch: 6/20... Training Step: 7600... Training loss: 0.8315... \n",
      "Epoch: 6/20... Training Step: 7800... Training loss: 0.8759... \n",
      "Epoch: 6/20... Training Step: 8000... Training loss: 0.8082... \n",
      "Epoch: 6/20... Training Step: 8200... Training loss: 0.8395... \n",
      "Epoch: 6/20... Training Step: 8400... Training loss: 0.8238... \n",
      "Epoch: 6/20... Training Step: 8600... Training loss: 0.8113... \n",
      "Epoch: 7/20... Training Step: 8800... Training loss: 0.8508... \n",
      "Epoch: 7/20... Training Step: 9000... Training loss: 0.8441... \n",
      "Epoch: 7/20... Training Step: 9200... Training loss: 0.8390... \n",
      "Epoch: 7/20... Training Step: 9400... Training loss: 0.8362... \n",
      "Epoch: 7/20... Training Step: 9600... Training loss: 0.8079... \n",
      "Epoch: 7/20... Training Step: 9800... Training loss: 0.8108... \n",
      "Epoch: 7/20... Training Step: 10000... Training loss: 0.7823... \n",
      "Epoch: 8/20... Training Step: 10200... Training loss: 0.8209... \n",
      "Epoch: 8/20... Training Step: 10400... Training loss: 0.8201... \n",
      "Epoch: 8/20... Training Step: 10600... Training loss: 0.7882... \n",
      "Epoch: 8/20... Training Step: 10800... Training loss: 0.7846... \n",
      "Epoch: 8/20... Training Step: 11000... Training loss: 0.7862... \n",
      "Epoch: 8/20... Training Step: 11200... Training loss: 0.8045... \n",
      "Epoch: 8/20... Training Step: 11400... Training loss: 0.7974... \n",
      "Epoch: 9/20... Training Step: 11600... Training loss: 0.7986... \n",
      "Epoch: 9/20... Training Step: 11800... Training loss: 0.7823... \n",
      "Epoch: 9/20... Training Step: 12000... Training loss: 0.8082... \n",
      "Epoch: 9/20... Training Step: 12200... Training loss: 0.7997... \n",
      "Epoch: 9/20... Training Step: 12400... Training loss: 0.7797... \n",
      "Epoch: 9/20... Training Step: 12600... Training loss: 0.7806... \n",
      "Epoch: 9/20... Training Step: 12800... Training loss: 0.7705... \n",
      "Epoch: 9/20... Training Step: 13000... Training loss: 0.7973... \n",
      "Epoch: 10/20... Training Step: 13200... Training loss: 0.7865... \n",
      "Epoch: 10/20... Training Step: 13400... Training loss: 0.7749... \n",
      "Epoch: 10/20... Training Step: 13600... Training loss: 0.7419... \n",
      "Epoch: 10/20... Training Step: 13800... Training loss: 0.7515... \n",
      "Epoch: 10/20... Training Step: 14000... Training loss: 0.7668... \n",
      "Epoch: 10/20... Training Step: 14200... Training loss: 0.7703... \n",
      "Epoch: 10/20... Training Step: 14400... Training loss: 0.7518... \n",
      "Epoch: 11/20... Training Step: 14600... Training loss: 0.7610... \n",
      "Epoch: 11/20... Training Step: 14800... Training loss: 0.7635... \n",
      "Epoch: 11/20... Training Step: 15000... Training loss: 0.7530... \n",
      "Epoch: 11/20... Training Step: 15200... Training loss: 0.7568... \n",
      "Epoch: 11/20... Training Step: 15400... Training loss: 0.7550... \n",
      "Epoch: 11/20... Training Step: 15600... Training loss: 0.7320... \n",
      "Epoch: 11/20... Training Step: 15800... Training loss: 0.7507... \n",
      "Epoch: 12/20... Training Step: 16000... Training loss: 0.7509... \n",
      "Epoch: 12/20... Training Step: 16200... Training loss: 0.7585... \n",
      "Epoch: 12/20... Training Step: 16400... Training loss: 0.7619... \n",
      "Epoch: 12/20... Training Step: 16600... Training loss: 0.7321... \n",
      "Epoch: 12/20... Training Step: 16800... Training loss: 0.7462... \n",
      "Epoch: 12/20... Training Step: 17000... Training loss: 0.7423... \n",
      "Epoch: 12/20... Training Step: 17200... Training loss: 0.7505... \n",
      "Epoch: 13/20... Training Step: 17400... Training loss: 0.7773... \n",
      "Epoch: 13/20... Training Step: 17600... Training loss: 0.7415... \n",
      "Epoch: 13/20... Training Step: 17800... Training loss: 0.7453... \n",
      "Epoch: 13/20... Training Step: 18000... Training loss: 0.7474... \n",
      "Epoch: 13/20... Training Step: 18200... Training loss: 0.7397... \n",
      "Epoch: 13/20... Training Step: 18400... Training loss: 0.7497... \n",
      "Epoch: 13/20... Training Step: 18600... Training loss: 0.7383... \n",
      "Epoch: 14/20... Training Step: 18800... Training loss: 0.7528... \n",
      "Epoch: 14/20... Training Step: 19000... Training loss: 0.7211... \n",
      "Epoch: 14/20... Training Step: 19200... Training loss: 0.7274... \n",
      "Epoch: 14/20... Training Step: 19400... Training loss: 0.7425... \n",
      "Epoch: 14/20... Training Step: 19600... Training loss: 0.7532... \n",
      "Epoch: 14/20... Training Step: 19800... Training loss: 0.7131... \n",
      "Epoch: 14/20... Training Step: 20000... Training loss: 0.7388... \n",
      "Epoch: 14/20... Training Step: 20200... Training loss: 0.7322... \n",
      "Epoch: 15/20... Training Step: 20400... Training loss: 0.7426... \n",
      "Epoch: 15/20... Training Step: 20600... Training loss: 0.7377... \n",
      "Epoch: 15/20... Training Step: 20800... Training loss: 0.7369... \n",
      "Epoch: 15/20... Training Step: 21000... Training loss: 0.7315... \n",
      "Epoch: 15/20... Training Step: 21200... Training loss: 0.7382... \n",
      "Epoch: 15/20... Training Step: 21400... Training loss: 0.7205... \n",
      "Epoch: 15/20... Training Step: 21600... Training loss: 0.7388... \n",
      "Epoch: 16/20... Training Step: 21800... Training loss: 0.7094... \n",
      "Epoch: 16/20... Training Step: 22000... Training loss: 0.7244... \n",
      "Epoch: 16/20... Training Step: 22200... Training loss: 0.7104... \n",
      "Epoch: 16/20... Training Step: 22400... Training loss: 0.7087... \n",
      "Epoch: 16/20... Training Step: 22600... Training loss: 0.7002... \n",
      "Epoch: 16/20... Training Step: 22800... Training loss: 0.7238... \n",
      "Epoch: 16/20... Training Step: 23000... Training loss: 0.6986... \n",
      "Epoch: 17/20... Training Step: 23200... Training loss: 0.6980... \n",
      "Epoch: 17/20... Training Step: 23400... Training loss: 0.7122... \n",
      "Epoch: 17/20... Training Step: 23600... Training loss: 0.7213... \n",
      "Epoch: 17/20... Training Step: 23800... Training loss: 0.7267... \n",
      "Epoch: 17/20... Training Step: 24000... Training loss: 0.7184... \n",
      "Epoch: 17/20... Training Step: 24200... Training loss: 0.6871... \n",
      "Epoch: 17/20... Training Step: 24400... Training loss: 0.7349... \n",
      "Epoch: 18/20... Training Step: 24600... Training loss: 0.7260... \n",
      "Epoch: 18/20... Training Step: 24800... Training loss: 0.7056... \n",
      "Epoch: 18/20... Training Step: 25000... Training loss: 0.7390... \n",
      "Epoch: 18/20... Training Step: 25200... Training loss: 0.7296... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20... Training Step: 25400... Training loss: 0.7149... \n",
      "Epoch: 18/20... Training Step: 25600... Training loss: 0.6997... \n",
      "Epoch: 18/20... Training Step: 25800... Training loss: 0.7127... \n",
      "Epoch: 18/20... Training Step: 26000... Training loss: 0.7284... \n",
      "Epoch: 19/20... Training Step: 26200... Training loss: 0.6939... \n",
      "Epoch: 19/20... Training Step: 26400... Training loss: 0.7198... \n",
      "Epoch: 19/20... Training Step: 26600... Training loss: 0.7105... \n",
      "Epoch: 19/20... Training Step: 26800... Training loss: 0.6949... \n",
      "Epoch: 19/20... Training Step: 27000... Training loss: 0.6938... \n",
      "Epoch: 19/20... Training Step: 27200... Training loss: 0.7066... \n",
      "Epoch: 19/20... Training Step: 27400... Training loss: 0.6961... \n",
      "Epoch: 20/20... Training Step: 27600... Training loss: 0.7192... \n",
      "Epoch: 20/20... Training Step: 27800... Training loss: 0.7095... \n",
      "Epoch: 20/20... Training Step: 28000... Training loss: 0.6997... \n",
      "Epoch: 20/20... Training Step: 28200... Training loss: 0.7224... \n",
      "Epoch: 20/20... Training Step: 28400... Training loss: 0.7039... \n",
      "Epoch: 20/20... Training Step: 28600... Training loss: 0.7035... \n",
      "Epoch: 20/20... Training Step: 28800... Training loss: 0.7256... \n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #uncomment the following line to restore checkpoint\n",
    "    #saver.restore(sess, 'checkpoints/i28900_l128.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(one_hot_encoded, 256):\n",
    "            counter += 1\n",
    "            feed_dict = {model.inputs: x,\n",
    "                         model.targets: y,\n",
    "                         model.keep_prob: keep_prob,\n",
    "                         model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss,\n",
    "                                                 model.final_state,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict = feed_dict)\n",
    "            if (counter % 200 == 0):\n",
    "                print('Epoch: {}/{}...'.format(e+1, epochs),\n",
    "                      'Training Step: {}...'.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss))\n",
    "            if (counter % save_freq == 0):\n",
    "                saver.save(sess, 'checkpoints_v3/i{}_l{}.ckpt'.format(counter, 128)) #lstm_size\n",
    "        saver.save(sess, 'checkpoints_v3/i{}_l{}.ckpt'.format(counter, 128)) #lstm size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints_v3/i28900_l128.ckpt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_v3/i28900_l128.ckpt\n",
      "predecasse\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints_v3/i28900_l128.ckpt\n",
      "execnlitlan\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints_v3/i28900_l128.ckpt\n",
      "nomenclintes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sample(checkpoint, max_samples, lstm_size, num_classes, prime = 'a'):\n",
    "max_samples = 30\n",
    "samp = sample(checkpoint, max_samples, num_classes, prime = 'predeca')\n",
    "print(samp)\n",
    "samp = sample(checkpoint, max_samples, num_classes, prime = 'exec')\n",
    "print(samp)\n",
    "samp = sample(checkpoint, max_samples, num_classes, prime = 'nomencl')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we can extract the feature of each word, these features could propably be useful in future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(checkpoint, num_classes, prime = 'a'):\n",
    "    model = VocabRNN(learning_rate = learning_rate, sampling = True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            #x = np.zeros((1,1))\n",
    "            x = np.array([char_to_one_hot[c]]).reshape((1,1,-1))\n",
    "            feed_dict = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            features, new_state = sess.run([model.prediction, model.final_state], feed_dict = feed_dict)\n",
    "            features = features.flatten()\n",
    "            for s in new_state:\n",
    "                features = np.append(features, s.c.flatten())\n",
    "                features = np.append(features, s.h.flatten())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_v3/i28900_l128.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(795,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = get_feature(checkpoint, num_classes, prime = 'a')\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.89879793e-02  3.95795926e-02  3.60612459e-02  4.18184958e-02\n",
      "  4.17242423e-02  4.09130864e-02  3.47757377e-02  4.29286435e-02\n",
      "  3.14239264e-02  3.61453444e-02  2.76271738e-02  3.30974348e-02\n",
      "  4.53513265e-02  3.90439965e-02  5.00101224e-02  3.91344652e-02\n",
      "  3.57991941e-02  2.72507947e-02  3.78436446e-02  4.55554873e-02\n",
      "  4.70614284e-02  3.52772065e-02  3.18934396e-02  2.97207832e-02\n",
      "  2.68001761e-02  3.05545554e-02  3.36205065e-02 -4.83612865e-02\n",
      " -4.81348261e-02 -2.43934002e-02  3.53001654e-02 -3.81354950e-02\n",
      " -2.79355068e-02  2.57578082e-02 -3.87167186e-02 -2.71251239e-02\n",
      " -6.50828481e-02 -1.29774302e-01 -1.51702268e-02 -6.42467232e-04\n",
      " -5.31159043e-02  5.60663827e-02  4.09913808e-02  4.13453169e-02\n",
      "  2.57574897e-02  2.63117719e-02  4.37232107e-03  6.00210391e-02\n",
      "  5.23426160e-02 -1.90418810e-02 -4.82846722e-02  3.97134274e-02\n",
      "  1.37419045e-01  4.54110280e-03 -5.55290431e-02  5.76084815e-02\n",
      " -7.20720291e-02  4.30800058e-02 -2.59220570e-01 -1.32795824e-02\n",
      "  4.23586458e-01 -1.12903873e-02  1.50535991e-02 -5.19944653e-02\n",
      "  8.49739369e-03 -6.21691346e-02  8.76699910e-02  1.77405849e-02\n",
      "  6.66443408e-02 -8.83299857e-03  1.75385159e-02  1.29025236e-01\n",
      "  4.06867303e-02 -3.37664112e-02 -3.20095234e-02 -1.04810931e-01\n",
      " -1.69273990e-03 -1.44956142e-01  3.83036137e-01 -2.88038403e-02\n",
      "  7.39712566e-02 -2.79369652e-02  1.77118793e-01 -3.23325619e-02\n",
      " -5.60441352e-02  2.63428409e-03  5.50022647e-02  4.44249243e-01\n",
      " -4.92756516e-02 -4.27775458e-02 -4.89415340e-02 -1.09682970e-01\n",
      " -1.78619698e-02 -2.69335173e-02 -1.20106461e-02 -3.94065753e-02\n",
      "  3.88150848e-02 -1.44919213e-02 -8.14909022e-03 -4.63712066e-02\n",
      "  2.53206976e-02  2.42763315e-03 -1.16086677e-02 -1.71372652e-01\n",
      "  6.59386292e-02  7.49888942e-02 -4.39569838e-02  5.10076359e-02\n",
      " -3.89472209e-02 -3.51358019e-02 -7.27205500e-02 -4.01578322e-02\n",
      " -4.27021265e-01  4.71457802e-02 -1.21080233e-02  1.18904017e-01\n",
      "  2.10846607e-02  1.33306654e-02 -1.76716194e-01 -1.48228034e-02\n",
      "  8.96657184e-02 -3.86139974e-02 -2.95035797e-03 -4.72737141e-02\n",
      " -2.35872772e-02  3.89161259e-02 -2.10991446e-02  9.14108288e-03\n",
      " -9.51099694e-02 -1.33033653e-04  5.11071123e-02  1.37421638e-02\n",
      "  1.04819991e-01  6.83741122e-02 -6.93246722e-02 -2.36112714e-01\n",
      " -2.29948778e-02 -4.58778106e-02 -2.86770444e-02 -5.33294789e-02\n",
      " -3.47013660e-02  1.86552834e-02  6.32208958e-02  2.22286209e-04\n",
      "  3.09490841e-02  1.82565618e-02 -1.20886825e-02 -3.79480869e-02\n",
      "  7.92436525e-02  7.88122341e-02  3.91552551e-03 -3.00359353e-02\n",
      " -1.42513052e-01 -6.46016747e-02 -2.18100846e-02 -2.47724764e-02\n",
      " -1.53700588e-02 -1.34803234e-02  1.85280722e-02 -1.67370308e-02\n",
      " -1.48643143e-02  1.29303038e-02 -2.26998702e-02 -1.08448211e-02\n",
      " -3.79769690e-02 -4.50593494e-02 -3.98970768e-03 -2.98305298e-04\n",
      " -2.75267530e-02  2.65478902e-02  1.75611600e-02  1.70399416e-02\n",
      "  8.22717696e-03  1.33085065e-02  2.25929497e-03  3.17073949e-02\n",
      "  2.67253686e-02 -5.08062169e-03 -2.34120321e-02  1.79297030e-02\n",
      "  8.73781219e-02  1.93761499e-03 -2.94243693e-02  2.49329060e-02\n",
      " -3.39671038e-02  2.05728635e-02 -1.26201242e-01 -4.24700929e-03\n",
      "  2.52948582e-01 -5.75823523e-03  4.25495161e-03 -2.78493986e-02\n",
      "  2.28013867e-03 -2.63297893e-02  4.22354192e-02  8.65004305e-03\n",
      "  3.52969579e-02 -4.02754499e-03  9.11046937e-03  5.48424125e-02\n",
      "  1.95210353e-02 -1.87350139e-02 -1.54506182e-02 -4.96507511e-02\n",
      " -7.35801004e-04 -9.08578783e-02  1.88539416e-01 -1.43927485e-02\n",
      "  2.97124311e-02 -1.62220243e-02  9.13146213e-02 -1.03454720e-02\n",
      " -3.05420589e-02  1.43156515e-03  2.96968576e-02  2.41067097e-01\n",
      " -2.36755256e-02 -1.92967393e-02 -2.84978934e-02 -6.55492321e-02\n",
      " -9.14988574e-03 -7.43567059e-03 -6.15227222e-03 -2.22556107e-02\n",
      "  1.82391871e-02 -7.41102360e-03 -3.67416628e-03 -1.56031959e-02\n",
      "  1.15834624e-02  1.14281557e-03 -2.84344237e-03 -1.01453044e-01\n",
      "  3.31539139e-02  3.86309847e-02 -2.46300045e-02  2.46995427e-02\n",
      " -1.87701304e-02 -1.93740502e-02 -2.25462522e-02 -1.69355851e-02\n",
      " -2.39696637e-01  2.33976152e-02 -5.94466040e-03  3.69024836e-02\n",
      "  1.11603672e-02  6.57409057e-03 -9.02876779e-02 -7.96181802e-03\n",
      "  2.13854890e-02 -1.00483783e-02 -1.52960815e-03 -2.38319263e-02\n",
      " -1.13374665e-02  1.75485220e-02 -7.67123932e-03  3.37901968e-03\n",
      " -5.13558388e-02 -6.57301425e-05  1.85432360e-02  6.41767168e-03\n",
      "  5.61262816e-02  3.41708250e-02 -3.44778150e-02 -1.27239048e-01\n",
      " -1.00312633e-02 -2.32088696e-02 -1.57368612e-02 -2.52685230e-02\n",
      " -1.44228609e-02  8.98449775e-03  3.64640392e-02  1.02265119e-04\n",
      "  8.65909364e-03  9.98367742e-03 -7.23817293e-03 -1.65692605e-02\n",
      "  4.52077575e-02  3.85924764e-02  2.01462978e-03 -1.63004026e-02\n",
      " -6.77189827e-02 -2.64597069e-02 -1.11862244e-02  5.47611862e-02\n",
      " -4.22366798e-01  1.41453557e-03 -8.10851604e-02 -5.59350327e-02\n",
      "  6.21861853e-02  8.36587623e-02 -1.38397530e-01 -3.18940431e-02\n",
      "  3.82587425e-02  6.75321966e-02  3.64452489e-02 -1.67582314e-02\n",
      "  1.30865559e-01 -3.55091020e-02  4.03694808e-03  8.16094726e-02\n",
      "  5.49812727e-02  3.28807384e-02 -3.84733528e-02  6.75758496e-02\n",
      "  3.41392718e-02 -9.52381864e-02 -9.30565149e-02  1.48138199e-02\n",
      " -2.51077980e-01  2.08188131e-01 -2.76038498e-01  3.44075561e-01\n",
      "  2.26071160e-02  6.33657649e-02 -4.35439087e-02  7.55805597e-02\n",
      " -7.03516044e-03 -2.20705479e-01  3.71160358e-02 -4.23540473e-01\n",
      "  9.12532285e-02  4.60792743e-02  1.23995535e-01 -4.48404886e-02\n",
      " -2.00199001e-02 -9.51134339e-02  1.59329753e-02 -1.28070889e-02\n",
      "  2.31770817e-02  2.14961246e-02 -4.56350856e-02  1.43643424e-01\n",
      "  2.61710137e-01  7.95975793e-03 -9.03528705e-02 -5.42551763e-02\n",
      "  4.04033586e-02  6.19533658e-03  7.62364715e-02  1.16881587e-01\n",
      " -1.54779600e-02 -4.51535024e-02  1.31186292e-01 -6.60199532e-03\n",
      " -8.81961510e-02 -9.04417112e-02  4.15370762e-01  2.45706365e-01\n",
      "  3.07610594e-02  5.34053370e-02 -3.81920844e-01  5.82277365e-02\n",
      "  2.35726982e-02  3.16821225e-02 -6.47770762e-02  1.77747801e-01\n",
      " -1.30819753e-01 -1.23544157e-01  5.23001030e-02  3.89334885e-03\n",
      "  1.98033512e-01  3.47657688e-02  4.41294350e-02  4.64818738e-02\n",
      " -5.09702303e-02 -3.59012559e-02  1.77052319e-01  2.97669545e-02\n",
      " -3.56840901e-03  1.18216267e-03  1.62988275e-01 -1.09199353e-01\n",
      "  4.55528051e-02 -3.95185575e-02  1.20618477e-01 -1.08127125e-01\n",
      "  1.60261214e-01 -6.93456903e-02 -2.23599561e-02 -1.31724760e-01\n",
      "  2.50032097e-01 -4.04220372e-01  2.91790199e-02 -9.48380213e-03\n",
      "  3.29033732e-02  8.58073533e-02 -3.08756441e-01  8.28995705e-02\n",
      " -1.38909593e-01 -6.64768592e-02  7.88228773e-03  1.45786165e-04\n",
      "  8.97498727e-02 -5.19026339e-01  4.84512635e-02 -6.80403709e-02\n",
      " -2.79800426e-02 -2.36920565e-02  1.01230033e-01  3.19717564e-02\n",
      " -2.36614570e-02  1.87017359e-02  4.74838577e-02 -5.42360311e-03\n",
      "  7.37148598e-02 -4.90935184e-02  1.62909463e-01 -3.85447452e-03\n",
      " -7.58458525e-02  7.75814801e-02  3.53023857e-02  1.53076407e-02\n",
      " -1.64736032e-01  3.26810201e-04 -4.22359519e-02 -2.89126951e-02\n",
      "  2.83263754e-02  3.63763720e-02 -6.20742366e-02 -1.34400437e-02\n",
      "  1.46172233e-02  3.10434215e-02  1.61375515e-02 -5.46702277e-03\n",
      "  5.83071969e-02 -1.80876590e-02  1.59151875e-03  3.70484665e-02\n",
      "  2.96556298e-02  1.63769256e-02 -7.87961949e-03  2.18249168e-02\n",
      "  1.76729895e-02 -3.80819850e-02 -4.20829095e-02  5.81519166e-03\n",
      " -1.44156978e-01  6.32279366e-02 -1.40587136e-01  1.53176576e-01\n",
      "  9.68733057e-03  2.89254710e-02 -1.10991290e-02  3.32021341e-02\n",
      " -2.03354703e-03 -1.44315004e-01  9.05510131e-03 -2.15240270e-01\n",
      "  4.84193377e-02  1.88851226e-02  3.66422348e-02 -1.18509009e-02\n",
      " -1.02137337e-02 -4.77834046e-02  8.93491693e-03 -3.01211537e-03\n",
      "  1.11985588e-02  5.33301197e-03 -1.20780915e-02  4.85973246e-02\n",
      "  2.00374708e-01  4.21683816e-03 -3.34849283e-02 -1.33940196e-02\n",
      "  1.97482593e-02  2.86740321e-03  2.88794823e-02  5.51376157e-02\n",
      " -6.50054589e-03 -1.04882419e-02  7.29273483e-02 -3.50697222e-03\n",
      " -4.66598123e-02 -2.37525329e-02  2.11463988e-01  1.05264083e-01\n",
      "  1.47968847e-02  2.78753452e-02 -1.69871375e-01  2.07730681e-02\n",
      "  1.05913980e-02  6.78623002e-03 -2.52061188e-02  5.94905578e-02\n",
      " -6.05258793e-02 -7.34203309e-02  2.16656681e-02  1.03293278e-03\n",
      "  1.21735424e-01  1.39439525e-02  1.01920031e-02  2.53403056e-02\n",
      " -2.87470166e-02 -7.97627866e-03  9.30593237e-02  1.49263348e-02\n",
      " -1.41853082e-03  5.72367862e-04  1.28034174e-01 -2.94489190e-02\n",
      "  1.17899673e-02 -1.74440425e-02  9.41076502e-02 -4.27118391e-02\n",
      "  6.59450442e-02 -2.27439012e-02 -1.12017468e-02 -5.97009175e-02\n",
      "  1.13650449e-01 -1.62955761e-01  1.38818193e-02 -2.38984660e-03\n",
      "  1.41007379e-02  4.09491770e-02 -1.44109234e-01  4.15067933e-02\n",
      " -1.04359485e-01 -2.07318589e-02  2.19241902e-03  3.67480170e-05\n",
      "  4.78572324e-02 -2.77301252e-01  2.17503048e-02 -3.75093743e-02\n",
      " -1.13036036e-02 -9.36005451e-03  3.92035283e-02  1.08762719e-02\n",
      " -9.46119148e-03  1.04460949e-02  2.77782418e-02 -3.01662786e-03\n",
      "  3.17385793e-02 -2.26791799e-02  6.96487203e-02 -1.75473141e-03\n",
      " -3.60704213e-02  2.40997113e-02  1.41432025e-02  8.20622221e-02\n",
      " -7.30121210e-02  1.40875680e-02 -5.06612360e-02  6.10418096e-02\n",
      " -4.54450436e-02 -1.48931146e-01  1.14542514e-01  9.09976885e-02\n",
      " -5.31584807e-02  9.60101373e-03 -4.98282611e-02 -6.08052574e-02\n",
      "  5.33992909e-02 -1.20996777e-02  1.65796950e-02 -1.28729522e-01\n",
      "  8.71178433e-02  9.30372775e-02  4.46096947e-03  8.26027170e-02\n",
      " -7.24077597e-02 -9.75845661e-03 -1.85995158e-02  4.62585278e-02\n",
      "  1.21599389e-03 -3.68972728e-03 -4.22406942e-03  1.73690561e-02\n",
      " -1.01253584e-01 -1.57884136e-01 -1.56477466e-01  7.53974915e-02\n",
      " -4.64576297e-02 -6.10609278e-02 -1.85254030e-04 -4.27945964e-02\n",
      " -9.41224843e-02 -3.17320339e-02  1.92340761e-02  9.73038226e-02\n",
      " -5.73587790e-02 -2.36704405e-02  1.23065678e-04  4.83298041e-02\n",
      "  4.96031567e-02  9.29564163e-02  5.81609942e-02 -1.06948480e-01\n",
      " -5.00212088e-02 -1.00764535e-01  3.15502696e-02  2.89242983e-01\n",
      "  2.52605751e-02 -9.91623849e-02 -6.05885056e-04  5.03046962e-04\n",
      "  1.39896176e-03  1.12528004e-01  5.01849018e-02 -1.40025169e-01\n",
      " -5.82665801e-02 -1.27380341e-02  1.65656134e-01  6.17959956e-03\n",
      "  1.10437714e-01  5.00528850e-02 -9.14481059e-02 -2.07088739e-02\n",
      "  8.83433372e-02 -6.27646397e-04 -5.08588552e-02 -1.20061956e-01\n",
      "  5.19979373e-02 -2.92874370e-02  2.50848591e-01 -2.87202708e-02\n",
      " -8.95037577e-02  5.17123565e-03  8.34511071e-02 -3.28745805e-02\n",
      " -2.39848405e-01 -1.03331454e-01  1.61005501e-02 -1.00869812e-01\n",
      " -3.51071618e-02  1.65230244e-01 -1.56303525e-01  5.34644276e-02\n",
      " -7.95299560e-02  7.39950091e-02  4.40284004e-03  1.16314143e-02\n",
      "  1.12813264e-02  4.79813665e-02  1.02913857e-01 -7.90365934e-02\n",
      " -1.40790835e-01 -6.96283504e-02 -1.11272894e-01  1.02392763e-01\n",
      " -1.23876989e-01 -6.10840842e-02 -7.29556978e-02 -5.80085814e-02\n",
      " -7.02923015e-02 -2.94471327e-02  3.07070892e-02 -5.11589274e-02\n",
      " -1.22935325e-01 -1.76169779e-02  1.94645733e-01 -3.51479985e-02\n",
      "  6.79051317e-03  7.52701834e-02  4.40045111e-02 -2.04377342e-02\n",
      "  1.05125271e-01  3.79406065e-02  1.80519745e-02  4.72149141e-02\n",
      " -4.61691245e-03  7.67986849e-02  3.49956118e-02 -4.93738502e-02\n",
      "  2.18421258e-02 -6.53930902e-02  1.04756571e-01  4.74270172e-02\n",
      " -2.87565980e-02  5.88043034e-03 -5.58245182e-03  1.87666565e-02\n",
      " -1.28241619e-02 -6.95296898e-02  1.28798522e-02  2.46711131e-02\n",
      " -3.10415532e-02  4.04216582e-03 -1.40977139e-02 -1.62702873e-02\n",
      "  1.68745760e-02 -2.30014604e-03  4.79124673e-03 -2.57656425e-02\n",
      "  4.42188084e-02  3.39128524e-02  9.39274556e-04  1.56572945e-02\n",
      " -1.22470194e-02 -2.23477581e-03 -5.61290141e-03  2.11379752e-02\n",
      "  4.35578608e-04 -1.21845561e-03 -1.14347984e-03  3.46365222e-03\n",
      " -3.35618556e-02 -7.25551620e-02 -7.17766583e-02  1.43360849e-02\n",
      " -9.85518657e-03 -9.50045139e-03 -4.49228464e-05 -2.42221113e-02\n",
      " -3.57228331e-02 -1.01824533e-02  5.36260242e-03  2.34649535e-02\n",
      " -5.15270140e-03 -8.18628073e-03  6.55582480e-05  5.28923795e-03\n",
      "  1.28841791e-02  1.04115847e-02  9.62405745e-03 -4.63038012e-02\n",
      " -1.05431704e-02 -4.33639549e-02  1.91660114e-02  1.46075532e-01\n",
      "  1.38748102e-02 -7.81081384e-03 -1.45454207e-04  1.88307458e-04\n",
      "  4.92725696e-04  6.58993647e-02  1.55706191e-02 -6.67522028e-02\n",
      " -1.21670915e-02 -6.48790691e-03  6.97330311e-02  1.08605565e-03\n",
      "  5.84826209e-02  1.43366409e-02 -1.28255263e-02 -9.21550859e-03\n",
      "  3.18868794e-02 -1.07602900e-04 -1.26832062e-02 -2.10007504e-02\n",
      "  1.61685720e-02 -1.68687534e-02  6.64249510e-02 -9.69356485e-03\n",
      " -4.40333635e-02  3.19853122e-03  2.12459303e-02 -1.22804847e-02\n",
      " -1.55138478e-01 -2.28319764e-02  6.38947031e-03 -1.14783132e-02\n",
      " -5.93608757e-03  5.47801629e-02 -8.34327936e-02  1.58409793e-02\n",
      " -1.63405966e-02  3.31935249e-02  1.19570666e-03  7.29793916e-03\n",
      "  3.80929909e-03  2.39427146e-02  9.58762690e-03 -4.68984693e-02\n",
      " -7.27486685e-02 -3.65869403e-02 -7.64047652e-02  2.43115779e-02\n",
      " -1.26947034e-02 -1.61047690e-02 -3.04181036e-02 -2.39694994e-02\n",
      " -2.40157321e-02 -1.17442710e-02  1.49742402e-02 -2.46299896e-02\n",
      " -3.34422290e-02 -1.04373517e-02  8.36737230e-02 -9.57717653e-03\n",
      "  2.73870514e-03  3.16138864e-02  1.12947300e-02 -9.76482872e-03\n",
      "  3.63300331e-02  1.79895256e-02  5.24656568e-03  1.13701373e-02\n",
      " -1.43142894e-03  2.04602946e-02  9.23287217e-03 -3.95004079e-03\n",
      "  4.34688432e-03 -2.83165462e-02  3.87608819e-02]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
